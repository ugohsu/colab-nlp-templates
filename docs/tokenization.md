# å½¢æ…‹ç´ è§£æã‚’è¡Œã†é–¢æ•°ï¼ˆJanome / SudachiPyï¼‰

æœ¬ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã§ã¯ã€Pandas DataFrame ã«æ ¼ç´ã•ã‚ŒãŸæ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆã‚’å¯¾è±¡ã¨ã—ã¦ã€  
**Janome** ã¾ãŸã¯ **SudachiPy** ã‚’ç”¨ã„ãŸå½¢æ…‹ç´ è§£æã‚’è¡Œã„ã€  
ã€Œ**1è¡Œ = 1ãƒˆãƒ¼ã‚¯ãƒ³**ã€ã®ç¸¦æŒã¡ DataFrame ã«å¤‰æ›ã™ã‚‹é–¢æ•°ç¾¤ã«ã¤ã„ã¦èª¬æ˜ã—ã¾ã™ã€‚

ã“ã‚Œã‚‰ã®é–¢æ•°ã¯ **ãƒ©ã‚¤ãƒ–ãƒ©ãƒªåŒ–**ã•ã‚Œã¦ãŠã‚Šã€  
Google Colaboratory ä¸Šã§ã¯æœ¬ãƒªãƒã‚¸ãƒˆãƒªã‚’ clone ã—ãŸã†ãˆã§ **import ã—ã¦ä½¿ç”¨**ã—ã¾ã™ã€‚

ã„ãšã‚Œã‚‚ **æ–‡æ›¸IDåˆ—ã‚’å¿…é ˆ**ã¨ã™ã‚‹è¨­è¨ˆã§ã‚ã‚Šã€  
LDA / NMF / TF-IDF / WordCloud / èªé »åº¦é›†è¨ˆãªã©ã®åˆ†æå‡¦ç†ã«  
ãã®ã¾ã¾æ¥ç¶šã§ãã‚‹ã“ã¨ã‚’é‡è¦–ã—ã¦ã„ã¾ã™ã€‚

---

## Google Colab ã§ã®ä½¿ã„æ–¹ï¼ˆé‡è¦ï¼‰

### 0. ã©ã¡ã‚‰ã‚’ä½¿ã†ã‹æ±ºã‚ã‚‹

æœ¬ãƒªãƒã‚¸ãƒˆãƒªã§ã¯ã€ä»¥ä¸‹ã® **2 ç¨®é¡ã®å½¢æ…‹ç´ è§£æã‚¨ãƒ³ã‚¸ãƒ³**ã‚’æä¾›ã—ã¦ã„ã¾ã™ã€‚

- **Janome ç‰ˆ**ï¼šè»½é‡ãƒ»å°å…¥ãŒç°¡å˜ï¼ˆæˆæ¥­ãƒ»æ¼”ç¿’å‘ã‘ï¼‰
- **SudachiPy ç‰ˆ**ï¼šé«˜ç²¾åº¦ãƒ»é«˜æ©Ÿèƒ½ï¼ˆç ”ç©¶ç”¨é€”å‘ã‘ï¼‰

ğŸ‘‰ **é€šå¸¸ã¯ã©ã¡ã‚‰ã‹ä¸€æ–¹ã ã‘ã‚’ä½¿ãˆã°ååˆ†ã§ã™ã€‚**  
Janome ã¨ SudachiPy ã‚’ **åŒæ™‚ã«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹å¿…è¦ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚**

---

### 1-A. Janome ã‚’ä½¿ã†å ´åˆï¼ˆãŠã™ã™ã‚ï¼šæˆæ¥­ãƒ»æ¼”ç¿’ï¼‰

```python
!pip install janome
from libs import tokenize_df
```

---

### 1-B. SudachiPy ã‚’ä½¿ã†å ´åˆï¼ˆç ”ç©¶ç”¨é€”ãƒ»ç²¾åº¦é‡è¦–ï¼‰

```python
!pip install sudachipy sudachidict_core
from libs import tokenize_df
```

---

â€» **Google Colab ã§ã¯ã€ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã¯å¿…ãš `!pip install ...` ã®å½¢å¼ã§å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚**  
â€» `pip install ...`ï¼ˆ!ãªã—ï¼‰ã¯ Colab ã§ã¯å‹•ä½œã—ã¾ã›ã‚“ã€‚

---

# 1. ã¾ãšå…¨ä½“åƒã‚’ç†è§£ã™ã‚‹

## 1.1 ãªãœã€Œå½¢æ…‹ç´ è§£æã€ãŒå¿…è¦ãªã®ã‹

æ—¥æœ¬èªã®æ–‡ç« ã¯ã€è‹±èªã®ã‚ˆã†ã«å˜èªã”ã¨ã«ç©ºç™½ã§åŒºåˆ‡ã‚‰ã‚Œã¦ã„ã¾ã›ã‚“ã€‚

ä¾‹ï¼š
> ä»Šæ—¥ã¯è‰¯ã„å¤©æ°—ã§ã—ãŸã€‚

ã“ã®æ–‡ã‚’ãã®ã¾ã¾ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã«æ¸¡ã—ã¦ã‚‚ã€
- ã€Œä»Šæ—¥ã¯ã€
- ã€Œè‰¯ã„ã€
- ã€Œå¤©æ°—ã€
- ã€Œã§ã—ãŸã€

ã¨ã„ã£ãŸ **æ„å‘³ã®ã‚ã‚‹å˜ä½ï¼ˆèªï¼‰** ã‚’è‡ªå‹•ã§åˆ‡ã‚Šå‡ºã™ã“ã¨ãŒã§ãã¾ã›ã‚“ã€‚

ãã“ã§å¿…è¦ã«ãªã‚‹ã®ãŒ **å½¢æ…‹ç´ è§£æ** ã§ã™ã€‚

å½¢æ…‹ç´ è§£æã¨ã¯ã€
> æ–‡ã‚’ã€Œã“ã‚Œä»¥ä¸Šåˆ†è§£ã™ã‚‹ã¨æ„å‘³ã‚’å¤±ã†æœ€å°å˜ä½ï¼ˆå½¢æ…‹ç´ ï¼‰ã€ã«åˆ†å‰²ã—ã€  
> ãã‚Œãã‚Œã«å“è©ãªã©ã®æƒ…å ±ã‚’ä»˜ä¸ã™ã‚‹å‡¦ç†

ã§ã™ã€‚

---

## 1.2 ã€Œè¡¨å±¤ï¼ˆã²ã‚‡ã†ãã†ï¼‰ã€ã¨ã¯ä½•ã‹

å½¢æ…‹ç´ è§£æã§ã¯ã€1ã¤ã®èªï¼ˆãƒˆãƒ¼ã‚¯ãƒ³ï¼‰ã«å¯¾ã—ã¦ã€è¤‡æ•°ã®ã€Œèªã®è¡¨ã—æ–¹ã€ãŒã‚ã‚Šã¾ã™ã€‚

ä¾‹ï¼š
> é£Ÿã¹ãŸ

ã“ã®ã¨ãã€

- **è¡¨å±¤å½¢ï¼ˆsurfaceï¼‰**ï¼šæ–‡ç« ã«å®Ÿéš›ã«ç¾ã‚Œã¦ã„ã‚‹æ–‡å­—åˆ—ï¼ˆã“ã®ä¾‹ã§ã¯ã€Œé£Ÿã¹ãŸã€ï¼‰
- **åŸºæœ¬å½¢ï¼ˆåŸå½¢ / è¾æ›¸å½¢ï¼‰**ï¼šè¾æ›¸ã«è¼‰ã£ã¦ã„ã‚‹å½¢ï¼ˆã“ã®ä¾‹ã§ã¯ã€Œé£Ÿã¹ã‚‹ã€ï¼‰

ã¨ã„ã†2ã¤ã‚’åŒºåˆ¥ã—ã¾ã™ã€‚

- ã€Œè¡¨å±¤ã€ã¯ **æ–‡ç« ã®è¡¨é¢ã«è¦‹ãˆã¦ã„ã‚‹å½¢**ï¼ˆãã®ã¾ã¾ã®æ–‡å­—åˆ—ï¼‰
- ã€ŒåŸºæœ¬å½¢ã€ã¯ **æ´»ç”¨ãªã©ã‚’å…ƒã«æˆ»ã—ãŸå½¢**

ã§ã™ã€‚

---

## 1.3 ãªãœã€Œç¸¦æŒã¡ DataFrameã€ã«ã™ã‚‹ã®ã‹

æœ¬ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã¯ã€å½¢æ…‹ç´ è§£æã®çµæœã‚’æ¬¡ã®ã‚ˆã†ãªå½¢ã§æ‰±ã„ã¾ã™ã€‚

| article_id | word | pos |
|-----------|------|-----|
| 1 | ä»Šæ—¥ | åè© |
| 1 | è‰¯ã„ | å½¢å®¹è© |
| 1 | å¤©æ°— | åè© |
| 1 | ã  | åŠ©å‹•è© |

ã“ã®ã‚ˆã†ã«  
**ã€Œ1è¡Œ = 1ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆèªï¼‰ã€**  
ã¨ã„ã†å½¢å¼ã‚’ã€Œç¸¦æŒã¡ã€ã¨å‘¼ã³ã¾ã™ã€‚

### ç¸¦æŒã¡ã®åˆ©ç‚¹

- èªã®å‡ºç¾é »åº¦ï¼ˆcountï¼‰ã‚’æ•°ãˆã‚„ã™ã„
- å“è©ãƒ•ã‚£ãƒ«ã‚¿ãŒç°¡å˜ï¼ˆ`df[df["pos"]=="åè©"]` ã®ã‚ˆã†ã«æ›¸ã‘ã‚‹ï¼‰
- LDA / TF-IDF / WordCloud ã«ã¤ãªãŒã‚‹å‰å‡¦ç†ãŒä½œã‚Šã‚„ã™ã„
- pandas ã® `groupby`, `value_counts` ãŒãã®ã¾ã¾ä½¿ãˆã‚‹

---

# 2. ã™ã™ã‚æ–¹ï¼šã¾ãš tokenizeã€ã‚ã¨ã§ãƒ•ã‚£ãƒ«ã‚¿

ã„ããªã‚Šã€Œåè©ã ã‘ã€ãªã©ã«é™å®šã—ã¦ã—ã¾ã†ã¨ã€  
ä½•ãŒæ¶ˆãˆãŸã®ã‹ï¼ä½•ãŒæ®‹ã£ãŸã®ã‹ãŒåˆ†ã‹ã‚Šã«ãããªã‚Šã¾ã™ã€‚

ãã“ã§ã€æ¬¡ã®é †åºã‚’ãŠã™ã™ã‚ã—ã¾ã™ã€‚

## Step 1ï¼šã¾ãšã¯å“è©ã‚’é™å®šã›ãš tokenize ã™ã‚‹ï¼ˆå…¨ãƒˆãƒ¼ã‚¯ãƒ³ï¼‰

```python
df_tok_all = tokenize_df(df)  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ Janome
```

## Step 2ï¼šçµæœã‚’è¦³å¯Ÿã™ã‚‹ï¼ˆé »å‡ºèªãƒ»å“è©ï¼‰

```python
df_tok_all["pos"].value_counts().head(20)
df_tok_all["word"].value_counts().head(20)
```

## Step 3ï¼šã‚ã¨ã‹ã‚‰ filter ã—ã¦ã„ãï¼ˆæ®µéšçš„ã«è½ã¨ã™ï¼‰

```python
from libs.preprocess import filter_tokens_df

# ã¾ãšã¯è¨˜å·ã‚„ç©ºç™½ã ã‘è½ã¨ã™
df_tok_1 = filter_tokens_df(df_tok_all, pos_exclude={"è£œåŠ©è¨˜å·", "ç©ºç™½"})

# æ¬¡ã«åŠ©è©ã‚‚è½ã¨ã™ï¼ˆç›®çš„ã«ã‚ˆã‚Šï¼‰
df_tok_2 = filter_tokens_df(df_tok_1, pos_exclude={"åŠ©è©"})

# stopword ã‚‚è½ã¨ã›ã‚‹
top10words = df_tok_2["word"].value_count().head(10) # å‡ºç¾é »åº¦ä¸Šä½10å˜èªã‚’ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ã¨ã—ã¦èªå®š
df_tok_3 = filter_tokens_df(df_tok_2, stopwords=(top10words, "ã„ã‚‹", "ã‚ã‚‹"))
```

---

# 3. é–¢æ•°ãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹ï¼ˆä¸å¯§ç‰ˆï¼‰

ã“ã“ã‹ã‚‰ã¯ã€ä»¥ä¸‹ã®4é–¢æ•°ã«ã¤ã„ã¦

- **å®Œå…¨ãªå¼•æ•°ä¸€è¦§ï¼ˆç¶²ç¾…ï¼‰**
- **å„å¼•æ•°ã®æ„å‘³**
- **ã‚ˆãã‚ã‚‹ä½¿ã„æ–¹ã®ãƒ¬ã‚·ãƒ”**
- **ã¤ã¾ãšããƒã‚¤ãƒ³ãƒˆ**

ã‚’ã¾ã¨ã‚ã¾ã™ã€‚

å¯¾è±¡é–¢æ•°ï¼š
- `tokenize_df`
- `filter_tokens_df`
- `tokenize_text_janome`
- `tokenize_text_sudachi`

ï¼ˆâ€»æœ¬èª¬æ˜ã¯ `preprocess.py` æ¸…æ›¸ç‰ˆã®ä»•æ§˜ã«åˆã‚ã›ã¦ã„ã¾ã™ã€‚ï¼‰

---

# 4. tokenize_dfï¼ˆå…¥å£é–¢æ•°ï¼‰

## 4.1 ä½•ã‚’ã™ã‚‹é–¢æ•°ã‹

`tokenize_df` ã¯ **DataFrameï¼ˆæ–‡æ›¸å˜ä½ï¼‰** ã‚’å—ã‘å–ã‚Šã€  
å½¢æ…‹ç´ è§£æã—ã¦ **DataFrameï¼ˆãƒˆãƒ¼ã‚¯ãƒ³å˜ä½ï¼‰** ã«å¤‰æ›ã—ã¾ã™ã€‚

- å…¥åŠ›ï¼š1è¡Œ=1æ–‡æ›¸ï¼ˆã¾ãŸã¯1è¨˜äº‹ï¼‰ã® DataFrame
- å‡ºåŠ›ï¼š1è¡Œ=1ãƒˆãƒ¼ã‚¯ãƒ³ã® DataFrame

---

## 4.2 ã‚·ã‚°ãƒãƒãƒ£ï¼ˆæ¦‚å¿µï¼‰

```python
tokenize_df(
    df,
    *,
    id_col="article_id",
    text_col="article",
    engine="janome",
    tokenizer=None,
    tokenize_text_fn=None,
    use_base_form=True,
    pos_keep=None,
    pos_exclude=None,
    stopwords=None,
    extra_col="token_info",
) -> pandas.DataFrame
```

ãƒã‚¤ãƒ³ãƒˆï¼š
- **ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ Janome**ï¼ˆå°å…¥ãŒç°¡å˜ã§è»½é‡ï¼‰
- Sudachi å›ºæœ‰ã® `split_mode` ã¯ `tokenize_df` ã§ã¯æ‰±ã„ã¾ã›ã‚“  
  â†’ å¤‰æ›´ã—ãŸã„å ´åˆã¯ `tokenize_text_fn` ã‚’ä½¿ã„ã¾ã™ï¼ˆå¾Œè¿°ï¼‰

---

## 4.3 å¼•æ•°ï¼ˆå®Œå…¨ç¶²ç¾…ï¼‰

### dfï¼ˆå¿…é ˆï¼‰
- å‹ï¼š`pandas.DataFrame`
- æ„å‘³ï¼šå…¥åŠ›ï¼ˆæ–‡æ›¸å˜ä½ï¼‰ã®è¡¨
- å¿…è¦ãªåˆ—ï¼š`id_col` ã¨ `text_col` ãŒå¿…ãšå­˜åœ¨ã™ã‚‹ã“ã¨

---

### id_col
- å‹ï¼š`str`
- æ—¢å®šï¼š`"article_id"`
- æ„å‘³ï¼šæ–‡æ›¸IDåˆ—åï¼ˆä¾‹ï¼šè¨˜äº‹IDã€ãƒ•ã‚¡ã‚¤ãƒ«IDã€è¡ŒIDï¼‰
- ä¾‹ï¼š`id_col="id"`

---

### text_col
- å‹ï¼š`str`
- æ—¢å®šï¼š`"article"`
- æ„å‘³ï¼šè§£æå¯¾è±¡ã®æœ¬æ–‡åˆ—å
- ä¾‹ï¼š`text_col="text"`

---

### engine
- å‹ï¼š`str`
- æ—¢å®šï¼š`"janome"`
- å–ã‚Šã†ã‚‹å€¤ï¼š`"janome"` / `"sudachi"`
- æ„å‘³ï¼šã©ã¡ã‚‰ã®å½¢æ…‹ç´ è§£æã‚¨ãƒ³ã‚¸ãƒ³ã‚’ä½¿ã†ã‹

---

### tokenizer
- å‹ï¼šã‚¨ãƒ³ã‚¸ãƒ³ä¾å­˜ï¼ˆJanome Tokenizer / Sudachi Tokenizerï¼‰
- æ—¢å®šï¼š`None`ï¼ˆé–¢æ•°å†…ã§è‡ªå‹•ç”Ÿæˆï¼‰
- æ„å‘³ï¼šå¤–ã§ç”Ÿæˆã—ãŸ tokenizer ã‚’æ¸¡ã—ãŸã„ã¨ãã«æŒ‡å®š

#### ã„ã¤ä½¿ã†ï¼Ÿ
- tokenize ã‚’ä½•å›ã‚‚ç¹°ã‚Šè¿”ã™ï¼ˆé€Ÿåº¦æ”¹å–„ï¼‰
- `tokenize_text_fn` ã‚’ä½œã‚‹ã¨ãã«ã€tokenizer ã‚’ã‚¯ãƒ­ãƒ¼ã‚¸ãƒ£ã«é–‰ã˜è¾¼ã‚ãŸã„

---

### tokenize_text_fnï¼ˆæ‹¡å¼µãƒã‚¤ãƒ³ãƒˆï¼‰
- å‹ï¼š`callable` ã¾ãŸã¯ `None`
- æ—¢å®šï¼š`None`

æ„å‘³ï¼š
- 1ãƒ†ã‚­ã‚¹ãƒˆï¼ˆ1æ–‡æ›¸ï¼‰ã‚’ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºã™ã‚‹å‡¦ç†ã‚’ã€Œä¸¸ã”ã¨å·®ã—æ›¿ãˆã‚‹ã€ãŸã‚ã®å¼•æ•°ã€‚

**æŒ‡å®šã—ãŸå ´åˆã¯ã€åŸå‰‡ã¨ã—ã¦ tokenize_text_fn ãŒæœ€å„ªå…ˆã§ã™ã€‚**

#### tokenize_text_fn ã®ä»•æ§˜ï¼ˆå¿…ãšå®ˆã‚‹ï¼‰

`tokenize_text_fn(text)` ã¯æ¬¡ã®å½¢å¼ã‚’è¿”ã—ã¾ã™ï¼š

- æˆ»ã‚Šå€¤ï¼š`list[tuple[word, pos, token_info]]`

1ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¡¨ã™ã‚¿ãƒ—ãƒ«ï¼š
- `word`ï¼š`str`ï¼ˆä¸‹æµã®åˆ†æã§ä½¿ã†èªï¼‰
- `pos`ï¼š`str`ï¼ˆå“è©ã®å¤§åˆ†é¡ï¼‰
- `token_info`ï¼š`dict` ã¾ãŸã¯ `None`ï¼ˆè¿½åŠ æƒ…å ±ã€‚è¦ã‚‰ãªã‘ã‚Œã° None ã§OKï¼‰

ä¾‹ï¼š
```python
[
  ("ä»Šæ—¥", "åè©", {"surface": "ä»Šæ—¥", "base_form": "ä»Šæ—¥"}),
  ("è‰¯ã„", "å½¢å®¹è©", {"surface": "è‰¯ã„", "base_form": "è‰¯ã„"}),
]
```

---

### use_base_formï¼ˆå…±é€šã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
- å‹ï¼š`bool`
- æ—¢å®šï¼š`True`
- æ„å‘³ï¼šã€Œword ã¨ã—ã¦åŸºæœ¬å½¢ã‚’ä½¿ã†ã‹ï¼è¡¨å±¤å½¢ã‚’ä½¿ã†ã‹ã€

| ã‚¨ãƒ³ã‚¸ãƒ³ | Trueï¼ˆæ—¢å®šï¼‰ | False |
|---|---|---|
| Janome | åŸå½¢ï¼ˆbase_formï¼‰ | è¡¨å±¤å½¢ï¼ˆsurfaceï¼‰ |
| Sudachi | è¾æ›¸å½¢ï¼ˆdictionary_formï¼‰ | è¡¨å±¤å½¢ï¼ˆsurfaceï¼‰ |

---

### pos_keep
- å‹ï¼š`iterable[str]` ã¾ãŸã¯ `None`
- æ—¢å®šï¼š`None`ï¼ˆé™å®šã—ãªã„ï¼‰
- æ„å‘³ï¼šæŒ‡å®šã—ãŸå“è©ï¼ˆå¤§åˆ†é¡ï¼‰ã®ãƒˆãƒ¼ã‚¯ãƒ³ã ã‘æ®‹ã™
- ä¾‹ï¼š`pos_keep={"åè©","å‹•è©","å½¢å®¹è©"}`

---

### pos_exclude
- å‹ï¼š`iterable[str]` ã¾ãŸã¯ `None`
- æ—¢å®šï¼š`None`ï¼ˆé™¤å¤–ã—ãªã„ï¼‰
- æ„å‘³ï¼šæŒ‡å®šã—ãŸå“è©ï¼ˆå¤§åˆ†é¡ï¼‰ã‚’é™¤å¤–
- ä¾‹ï¼š`pos_exclude={"åŠ©è©","è£œåŠ©è¨˜å·","ç©ºç™½"}`

---

### stopwords
- å‹ï¼š`iterable[str]` ã¾ãŸã¯ `None`
- æ—¢å®šï¼š`None`
- æ„å‘³ï¼šword ãŒ stopwords ã«å«ã¾ã‚Œã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ã‚’é™¤å¤–
- ä¾‹ï¼š`stopwords={"ã™ã‚‹","ã‚ã‚‹","ãªã‚‹"}`

---

### extra_col
- å‹ï¼š`str` ã¾ãŸã¯ `None`
- æ—¢å®šï¼š`"token_info"`
- æ„å‘³ï¼š
  - `None`ï¼šè¿½åŠ æƒ…å ±åˆ—ã‚’ä½œã‚‰ãªã„ï¼ˆè»½é‡åŒ–ï¼‰
  - æ–‡å­—åˆ—ï¼šãã®åˆ—åã§ token_info ã‚’æ ¼ç´

---

## 4.4 å‡ºåŠ› DataFrame ã®ä»•æ§˜

tokenize_df ã®å‡ºåŠ›ã«ã¯æ¬¡ã®åˆ—ãŒå«ã¾ã‚Œã¾ã™ã€‚

- `id_col`ï¼ˆä¾‹ï¼š`article_id`ï¼‰
- `word`
- `pos`
- `token_info`ï¼ˆextra_col ãŒ None ã®ã¨ãã¯ä½œã‚‰ã‚Œãªã„ï¼ã¾ãŸã¯å…¨ã¦ Noneï¼‰

---

## 4.5 ãƒ¬ã‚·ãƒ”

### ãƒ¬ã‚·ãƒ”1ï¼šã¾ãšå…¨éƒ¨ tokenizeï¼ˆæ¨å¥¨ï¼‰
```python
df_tok_all = tokenize_df(df, id_col="article_id", text_col="article")
```

### ãƒ¬ã‚·ãƒ”2ï¼šSudachi ã‚’ä½¿ã†
```python
df_tok_all = tokenize_df(df, engine="sudachi")
```

### ãƒ¬ã‚·ãƒ”3ï¼šå“è©ãƒ•ã‚£ãƒ«ã‚¿ã‚’â€œå¾Œã§â€ã‚„ã‚‹
```python
df_tok_all = tokenize_df(df, extra_col=None)
df_tok = filter_tokens_df(df_tok_all, pos_exclude={"åŠ©è©","è£œåŠ©è¨˜å·","ç©ºç™½"})
```

---

# 5. filter_tokens_dfï¼ˆå“è©ãƒ•ã‚£ãƒ«ã‚¿å°‚ç”¨ï¼‰

## 5.1 ä½•ã‚’ã™ã‚‹é–¢æ•°ã‹

`filter_tokens_df` ã¯ã€`tokenize_df` ãŒè¿”ã™ **ãƒˆãƒ¼ã‚¯ãƒ³å˜ä½ DataFrame** ã‚’å—ã‘å–ã‚Šã€  
ä¸»ã«æ¬¡ã®3ç¨®é¡ã®æ¡ä»¶ã§ãƒˆãƒ¼ã‚¯ãƒ³ã‚’é™¤å¤–ãƒ»æŠ½å‡ºã™ã‚‹ãŸã‚ã®é–¢æ•°ã§ã™ã€‚

- å“è©ã«ã‚ˆã‚‹ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆ`pos_keep`, `pos_exclude`ï¼‰
- ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ã«ã‚ˆã‚‹é™¤å¤–ï¼ˆ`stopwords`ï¼‰
- æŒ‡å®šã®çŸ›ç›¾ã‚’æ¤œå‡ºã™ã‚‹ãŸã‚ã®å®‰å…¨ãƒã‚§ãƒƒã‚¯ï¼ˆ`strict`ï¼‰

ã“ã®é–¢æ•°ã¯ **tokenize å¾Œã«æ®µéšçš„ã«é©ç”¨ã™ã‚‹ã“ã¨**ã‚’å‰æã«è¨­è¨ˆã•ã‚Œã¦ã„ã¾ã™ã€‚

---

## 5.2 ã‚·ã‚°ãƒãƒãƒ£ï¼ˆæ¦‚å¿µï¼‰

```python
filter_tokens_df(
    df,
    *,
    pos_keep=None,
    pos_exclude=None,
    stopwords=None,
    strict=True,
) -> pandas.DataFrame
```

---

## 5.3 å…¥åŠ› DataFrame ã®å‰æ

`df`ï¼ˆå…¥åŠ› DataFrameï¼‰ã¯ã€æœ€ä½é™æ¬¡ã®åˆ—ã‚’å«ã‚“ã§ã„ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

- `word`ï¼šãƒˆãƒ¼ã‚¯ãƒ³æ–‡å­—åˆ—
- `pos`ï¼šå“è©ï¼ˆå¤§åˆ†é¡ï¼‰

ã“ã‚Œã‚‰ã¯ `tokenize_df` ã®æ¨™æº–å‡ºåŠ›ã«å«ã¾ã‚Œã¾ã™ã€‚

---

## 5.4 pos_keepï¼ˆæ®‹ã™å“è©ã‚’æŒ‡å®šï¼‰

- å‹ï¼š`iterable[str]` ã¾ãŸã¯ `None`
- æ—¢å®šï¼š`None`ï¼ˆã™ã¹ã¦æ®‹ã™ï¼‰

æŒ‡å®šã—ãŸå ´åˆã€`df["pos"]` ãŒ **pos_keep ã«å«ã¾ã‚Œã‚‹è¡Œã ã‘** ãŒæ®‹ã‚Šã¾ã™ã€‚

```python
# åè©ã ã‘æ®‹ã™
df_noun = filter_tokens_df(df_tok, pos_keep={"åè©"})

# åè©ã¨å½¢å®¹è©ã‚’æ®‹ã™
df_noun_adj = filter_tokens_df(df_tok, pos_keep={"åè©", "å½¢å®¹è©"})
```

---

## 5.5 pos_excludeï¼ˆé™¤å¤–ã™ã‚‹å“è©ã‚’æŒ‡å®šï¼‰

- å‹ï¼š`iterable[str]` ã¾ãŸã¯ `None`
- æ—¢å®šï¼š`None`ï¼ˆé™¤å¤–ã—ãªã„ï¼‰

æŒ‡å®šã—ãŸå ´åˆã€`df["pos"]` ãŒ **pos_exclude ã«å«ã¾ã‚Œã‚‹è¡Œã¯é™¤å¤–**ã•ã‚Œã¾ã™ã€‚

```python
# åŠ©è©ãƒ»åŠ©å‹•è©ã‚’é™¤å¤–
df_no_particles = filter_tokens_df(
    df_tok,
    pos_exclude={"åŠ©è©", "åŠ©å‹•è©"}
)

# è¨˜å·é¡ã‚’é™¤å¤–
df_no_symbols = filter_tokens_df(
    df_tok,
    pos_exclude={"è£œåŠ©è¨˜å·", "è¨˜å·"}
)
```

---

## 5.6 pos_keep ã¨ pos_exclude ã‚’åŒæ™‚ã«ä½¿ã†å ´åˆï¼ˆstrictï¼‰

- å‹ï¼š`bool`
- æ—¢å®šï¼š`True`

`pos_keep` ã¨ `pos_exclude` ã‚’ **åŒæ™‚ã«æŒ‡å®šã—ãŸå ´åˆã®å®‰å…¨è£…ç½®**ã§ã™ã€‚

`strict=True` ã®ã¨ãï¼š

- ä¸¡è€…ã®é›†åˆãŒ **å®Œå…¨ã«ç„¡é–¢ä¿‚ï¼ˆå…±é€šè¦ç´ ãŒã‚¼ãƒ­ï¼‰** ã®å ´åˆã€  
  æ„å›³ã—ãªã„æŒ‡å®šã®å¯èƒ½æ€§ãŒé«˜ã„ãŸã‚ `ValueError` ã‚’å‡ºã—ã¾ã™ã€‚

```python
# strict=Trueï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
df_f = filter_tokens_df(
    df_tok,
    pos_keep={"åè©", "å‹•è©"},
    pos_exclude={"åŠ©è©", "è¨˜å·"},
)
```

ã“ã®ãƒã‚§ãƒƒã‚¯ã‚’ç„¡åŠ¹ã«ã—ãŸã„å ´åˆã¯ `strict=False` ã‚’æŒ‡å®šã—ã¾ã™ã€‚

```python
df_f = filter_tokens_df(
    df_tok,
    pos_keep={"åè©", "å‹•è©"},
    pos_exclude={"åŠ©è©", "è¨˜å·"},
    strict=False,
)
```

---

## 5.7 stopwordsï¼ˆã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ã«ã‚ˆã‚‹é™¤å¤–ï¼‰

- å‹ï¼šå¤šæ§˜ï¼ˆä¸‹è¨˜å‚ç…§ï¼‰ã¾ãŸã¯ `None`
- æ—¢å®šï¼š`None`

`stopwords` ã¯ **é™¤å¤–ã—ãŸã„èªã®é›†åˆ**ã¨ã—ã¦æ‰±ã‚ã‚Œã¾ã™ã€‚  
`df["word"]` ãŒ stopwords ã«å«ã¾ã‚Œã‚‹è¡Œã¯é™¤å¤–ã•ã‚Œã¾ã™ã€‚

### æŒ‡å®šã§ãã‚‹å½¢å¼

`stopwords` ã«ã¯ã€æ¬¡ã®ã‚ˆã†ãªå½¢å¼ã‚’ **ãã®ã¾ã¾æ¸¡ã›ã¾ã™**ã€‚

- å˜ä¸€ã®æ–‡å­—åˆ—  
  ```python
  stopwords="ã‚ã‚‹"
  ```
- æ–‡å­—åˆ—ã®ãƒªã‚¹ãƒˆ / ã‚¿ãƒ—ãƒ« / é›†åˆ  
  ```python
  stopwords=["ã‚ã‚‹", "ã„ã‚‹"]
  ```
- pandas.Seriesï¼ˆ`value_counts()` ã®çµæœãªã©ï¼‰  
  â†’ **index éƒ¨åˆ†**ãŒ stopwords ã¨ã—ã¦ä½¿ã‚ã‚Œã¾ã™
- pandas.Index
- ä¸Šè¨˜ã‚’å…¥ã‚Œå­ã«ã—ãŸæ§‹é€   
  ```python
  stopwords=(["ã‚ã‚‹", "ã„ã‚‹"], vc.head(10))
  ```

å†…éƒ¨ã§ã¯ `_normalize_stopwords` ã«ã‚ˆã‚Šè‡ªå‹•çš„ã«æ­£è¦åŒ–ã•ã‚Œã¾ã™ã€‚

### ä¾‹

```python
# é »å‡ºèªä¸Šä½ã‚’ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ã¨ã—ã¦é™¤å¤–
vc_top10 = df_tok["word"].value_counts().head(10)

df_f = filter_tokens_df(
    df_tok,
    stopwords=vc_top10
)

# æ‰‹å‹•æŒ‡å®š + é »å‡ºèªæŒ‡å®š
df_f = filter_tokens_df(
    df_tok,
    stopwords=(vc_top10, ["ã‚ã‚‹", "ã„ã‚‹"])
)
```

â€» `"ã‚ã‚‹"` ã‚’ãã®ã¾ã¾æ¸¡ã—ã¦ã‚‚ã€æ–‡å­—å˜ä½ã«åˆ†è§£ã•ã‚Œã‚‹ã“ã¨ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚

---

## 5.8 æ®µéšçš„ã«ãƒ•ã‚£ãƒ«ã‚¿ã™ã‚‹ä¾‹ï¼ˆæ¨å¥¨ï¼‰

```python
# 1) ã¾ãšå…¨éƒ¨ tokenize
df_tok_all = tokenize_df(df)

# 2) è¨˜å·ãƒ»åŠ©è©ã‚’é™¤å¤–
df_tok_1 = filter_tokens_df(
    df_tok_all,
    pos_exclude={"è£œåŠ©è¨˜å·", "ç©ºç™½", "åŠ©è©"}
)

# 3) é »å‡ºèª + æ‰‹å‹• stopwords ã‚’é™¤å¤–
vc_top10 = df_tok_1["word"].value_counts().head(10)

df_tok_2 = filter_tokens_df(
    df_tok_1,
    stopwords=(vc_top10, ["ã‚ã‚‹", "ã„ã‚‹"])
)
```

---

# 6. tokenize_text_janomeï¼ˆ1ãƒ†ã‚­ã‚¹ãƒˆç”¨ãƒ»Janomeï¼‰

## 6.1 ä½•ã‚’ã™ã‚‹é–¢æ•°ã‹

Janome ã§ 1ã¤ã®æ–‡å­—åˆ—ã‚’ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºã—ã€  
`(word, pos, token_info)` ã®é…åˆ—ã‚’è¿”ã—ã¾ã™ã€‚

---

## 6.2 ã‚·ã‚°ãƒãƒãƒ£ï¼ˆæ¦‚å¿µï¼‰

```python
tokenize_text_janome(
    text,
    *,
    tokenizer,
    use_base_form=True,
    extra_col="token_info",
) -> list[tuple[word, pos, token_info]]
```

---

## 6.3 å¼•æ•°ï¼ˆå®Œå…¨ç¶²ç¾…ï¼‰

### text
- å‹ï¼š`str`ï¼ˆNone/ç©ºæ–‡å­—ã®å ´åˆã‚‚ã‚ã‚Šãˆã‚‹ï¼‰
- æŒ™å‹•ï¼šNone/ç©ºã¯ `[]` ã‚’è¿”ã™

### tokenizerï¼ˆå¿…é ˆï¼‰
- å‹ï¼š`janome.tokenizer.Tokenizer`

### use_base_form
- tokenize_df ã¨åŒã˜

### extra_col
- tokenize_df ã¨åŒã˜ï¼ˆNone ã®å ´åˆ token_info ã‚’ä½œã‚‰ãªã„ï¼‰

---

# 7. tokenize_text_sudachiï¼ˆ1ãƒ†ã‚­ã‚¹ãƒˆç”¨ãƒ»Sudachiï¼‰

## 7.1 ä½•ã‚’ã™ã‚‹é–¢æ•°ã‹

Sudachi ã§ 1ã¤ã®æ–‡å­—åˆ—ã‚’ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºã—ã€  
`(word, pos, token_info)` ã®é…åˆ—ã‚’è¿”ã—ã¾ã™ã€‚

---

## 7.2 ã‚·ã‚°ãƒãƒãƒ£ï¼ˆæ¦‚å¿µï¼‰

```python
tokenize_text_sudachi(
    text,
    *,
    tokenizer,
    split_mode="C",
    word_form=None,
    use_base_form=True,
    extra_col="token_info",
) -> list[tuple[word, pos, token_info]]
```

---

## 7.3 å¼•æ•°ï¼ˆå®Œå…¨ç¶²ç¾…ï¼‰

### tokenizerï¼ˆå¿…é ˆï¼‰
- å‹ï¼šSudachi tokenizerï¼ˆ`dictionary.Dictionary().create()` ã®æˆ»ã‚Šå€¤ï¼‰

### split_modeï¼ˆSudachi å›ºæœ‰ï¼‰
- å‹ï¼š`str`
- æ—¢å®šï¼š`"C"`
- å–ã‚Šã†ã‚‹å€¤ï¼š`"A"`, `"B"`, `"C"`
- æ„å‘³ï¼šåˆ†å‰²ç²’åº¦ï¼ˆAãŒç´°ã‹ã„ã€CãŒç²—ã„â€¦ã¨ã„ã†ã‚¤ãƒ¡ãƒ¼ã‚¸ã§OKï¼‰

### word_formï¼ˆSudachi å›ºæœ‰ï¼šèªå½¢æŒ‡å®šï¼‰
- å‹ï¼š`str` ã¾ãŸã¯ `None`
- æ—¢å®šï¼š`None`
- æ„å‘³ï¼šæœ€çµ‚çš„ã« word ã¨ã—ã¦æ¡ç”¨ã™ã‚‹èªå½¢ã‚’æŒ‡å®š
- å–ã‚Šã†ã‚‹å€¤ï¼š
  - Noneï¼šuse_base_form ã«å¾“ã†ï¼ˆTrueâ†’dictionary_form / Falseâ†’surfaceï¼‰
  - `"dictionary"`ï¼šè¾æ›¸å½¢
  - `"surface"`ï¼šè¡¨å±¤å½¢
  - `"normalized"`ï¼šæ­£è¦åŒ–å½¢ï¼ˆè¡¨è¨˜ã‚†ã‚Œå¸åã«æœ‰ç”¨ï¼‰

---

## 7.4 ãƒ¬ã‚·ãƒ”ï¼šSudachi ã®æ­£è¦åŒ–å½¢ã‚’ä½¿ã†ï¼ˆword_form="normalized"ï¼‰

```python
from sudachipy import dictionary
from libs import tokenize_df
from libs.preprocess import tokenize_text_sudachi

tok = dictionary.Dictionary().create()

df_tok = tokenize_df(
    df,
    tokenize_text_fn=lambda t: tokenize_text_sudachi(
        t,
        tokenizer=tok,
        split_mode="C",
        word_form="normalized",
        extra_col=None,
    ),
    extra_col=None,
)
```

---

## 7.5 ãƒ¬ã‚·ãƒ”ï¼šSudachi ã® split_mode ã‚’å¤‰ãˆã‚‹ï¼ˆtokenize_text_fn ã‚’ä½¿ã†ï¼‰

`split_mode` ã¯ `tokenize_df` ã§ã¯å—ã‘å–ã‚Šã¾ã›ã‚“ã€‚  
å¤‰ãˆãŸã„å ´åˆã¯ã€æ¬¡ã®ã‚ˆã†ã« `tokenize_text_fn` çµŒç”±ã§æŒ‡å®šã—ã¾ã™ã€‚

```python
from sudachipy import dictionary
from libs import tokenize_df
from libs.preprocess import tokenize_text_sudachi

tok = dictionary.Dictionary().create()

df_tok_A = tokenize_df(
    df,
    tokenize_text_fn=lambda t: tokenize_text_sudachi(
        t, tokenizer=tok, split_mode="A", word_form=None, extra_col=None
    ),
    extra_col=None,
)

df_tok_C = tokenize_df(
    df,
    tokenize_text_fn=lambda t: tokenize_text_sudachi(
        t, tokenizer=tok, split_mode="C", word_form=None, extra_col=None
    ),
    extra_col=None,
)
```

---

# 8. tokenize_text_fn ã‚’è‡ªä½œã™ã‚‹ï¼ˆMeCab ãªã©ï¼‰

## 8.1 tokenize_text_fn ã®å…¥åŠ›ä»•æ§˜

- å…¥åŠ›ï¼š`text`ï¼ˆ1æ–‡æ›¸åˆ†ã®æ–‡å­—åˆ—ï¼‰
- å¯èƒ½æ€§ï¼šNone / ç©ºæ–‡å­—ãŒæ¥ã‚‹ã“ã¨ãŒã‚ã‚‹
- æ¨å¥¨ï¼šNone/ç©ºãªã‚‰ `[]` ã‚’è¿”ã™

---

## 8.2 tokenize_text_fn ã®å‡ºåŠ›ä»•æ§˜ï¼ˆæœ€é‡è¦ï¼‰

æˆ»ã‚Šå€¤ã¯ **list** ã§ã€è¦ç´ ã¯ **3è¦ç´ ã‚¿ãƒ—ãƒ«**ã§ã™ã€‚

```python
[
  (word, pos, token_info),
  ...
]
```

- `word`ï¼š`str`
- `pos`ï¼š`str`ï¼ˆå“è©ã®å¤§åˆ†é¡ï¼‰
- `token_info`ï¼š`dict` ã¾ãŸã¯ `None`

---

## 8.3 MeCab ã®ä¾‹ï¼ˆé››å½¢ï¼‰

```python
def tokenize_text_mecab(text):
    if text is None:
        return []
    s = str(text).strip()
    if s == "":
        return []

    records = []
    # ã“ã“ã§ MeCab è§£æã—ã€å„å½¢æ…‹ç´ ã«ã¤ã„ã¦ï¼š
    #   word = ...
    #   pos  = ...
    #   token_info = {...} ã¾ãŸã¯ None
    # ã‚’ä½œã£ã¦ records.append((word, pos, token_info)) ã™ã‚‹
    return records

df_tok = tokenize_df(
    df,
    tokenize_text_fn=tokenize_text_mecab,
    extra_col=None,
)
```

---

# 9. ãƒ¦ãƒ¼ã‚¶ãƒ¼å®šç¾©è¾æ›¸

å½¢æ…‹ç´ è§£æã§ã¯ã€æ—¢å­˜ã®è¾æ›¸ã«å«ã¾ã‚Œã¦ã„ãªã„å°‚é–€ç”¨èªãƒ»å›ºæœ‰åè©ãƒ»è¤‡åˆèªãªã©ã‚’  
**1 èªã¨ã—ã¦æ‰±ã„ãŸã„** å ´é¢ãŒã‚ˆãã‚ã‚Šã¾ã™ã€‚ãã®ãŸã‚ã«åˆ©ç”¨ã™ã‚‹ã®ãŒ  
**ãƒ¦ãƒ¼ã‚¶ãƒ¼å®šç¾©è¾æ›¸**ã§ã™ã€‚

æœ¬ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼å®šç¾©è¾æ›¸ã®æ‰±ã„ã«ã¤ã„ã¦æ¬¡ã®æ–¹é‡ã‚’æ¡ã£ã¦ã„ã¾ã™ã€‚

- ãƒ¦ãƒ¼ã‚¶ãƒ¼å®šç¾©è¾æ›¸ã®æŒ‡å®šã‚„ç®¡ç†ã¯ **Janome / SudachiPy å´ã§è¡Œã†**
- tokenize_df ã¯ **ä½œæˆæ¸ˆã¿ã® tokenizer ã‚’ãã®ã¾ã¾ä½¿ã†**
- è¿½åŠ è¨­å®šãŒå¿…è¦ãªå ´åˆã¯ **tokenize_text_fn ã§æ˜ç¤ºçš„ã«åˆ¶å¾¡ã™ã‚‹**

ã“ã®ãŸã‚ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼å®šç¾©è¾æ›¸ã‚’ä½¿ã†å ´åˆã§ã‚‚  
**preprocess.py ã‚’å¤‰æ›´ã™ã‚‹å¿…è¦ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚**

---

## 9.1 åŸºæœ¬çš„ãªè€ƒãˆæ–¹

tokenize_df ã¯ã€Œå…¥å£é–¢æ•°ã€ã¨ã—ã¦è¨­è¨ˆã•ã‚Œã¦ãŠã‚Šã€  
å½¢æ…‹ç´ è§£æã‚¨ãƒ³ã‚¸ãƒ³å›ºæœ‰ã®è©³ç´°è¨­å®šã¯æ‰±ã„ã¾ã›ã‚“ã€‚

å½¹å‰²åˆ†æ‹…ã¯æ¬¡ã®ã¨ãŠã‚Šã§ã™ã€‚

- **è¾æ›¸ãƒ»è©³ç´°è¨­å®š**  
  â†’ Janome / SudachiPy ã§ tokenizer ã‚’ä½œæˆã™ã‚‹æ®µéšã§æŒ‡å®š
- **DataFrame å…¨ä½“ã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºå‡¦ç†**  
  â†’ tokenize_df ãŒæ‹…å½“

ã—ãŸãŒã£ã¦ã€

> tokenizer ã‚’è‡ªåˆ†ã§ä½œã£ã¦ tokenize_df ã«æ¸¡ã›ã°ã€  
> ãã®è¨­å®šï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼å®šç¾©è¾æ›¸ã‚’å«ã‚€ï¼‰ãŒãã®ã¾ã¾åæ˜ ã•ã‚Œã‚‹

ã¨ã„ã†æ§‹é€ ã«ãªã£ã¦ã„ã¾ã™ã€‚

---

## 9.2 Janome ã§ãƒ¦ãƒ¼ã‚¶ãƒ¼å®šç¾©è¾æ›¸ã‚’ä½¿ã†

Janome ã§ã¯ã€CSV å½¢å¼ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼è¾æ›¸ã‚’æŒ‡å®šã—ã¦ Tokenizer ã‚’ä½œæˆã§ãã¾ã™ã€‚

```python
from janome.tokenizer import Tokenizer
from libs import tokenize_df

# 1. ãƒ¦ãƒ¼ã‚¶ãƒ¼è¾æ›¸ã‚’æŒ‡å®šã—ã¦ Tokenizer ã‚’ä½œæˆ
my_tokenizer = Tokenizer(
    udic_path="user_dictionary.csv",
    udic_enc="utf8"
)

# 2. ä½œæˆã—ãŸ tokenizer ã‚’ tokenize_df ã«æ¸¡ã™
df_tokens = tokenize_df(
    df,
    engine="janome",
    tokenizer=my_tokenizer
)
```

---

## 9.3 SudachiPy ã§ãƒ¦ãƒ¼ã‚¶ãƒ¼å®šç¾©è¾æ›¸ã‚’ä½¿ã†

SudachiPy ã§ã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼è¾æ›¸ã‚„æ­£è¦åŒ–è¨­å®šã‚’  
**è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆsudachi.jsonï¼‰ã§ã¾ã¨ã‚ã¦ç®¡ç†ã™ã‚‹**ã®ãŒä¸€èˆ¬çš„ã§ã™ã€‚

```python
from sudachipy import dictionary
from libs import tokenize_df

my_tokenizer = dictionary.Dictionary(
    config_path="path/to/sudachi.json"
).create()

df_tokens = tokenize_df(
    df,
    engine="sudachi",
    tokenizer=my_tokenizer
)
```

Sudachi ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼è¾æ›¸ã®ä½œæˆæ–¹æ³•ã«ã¤ã„ã¦ã¯ã€[å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆï¼ˆ2025-12-26æ™‚ç‚¹ï¼‰](Sudachi_user_dict.md) ã‚’å‚ç…§ã—ã¾ã—ã‚‡ã†ã€‚

---

## 9.4 tokenize_text_fn ã‚’ä½¿ã£ã¦åˆ¶å¾¡ã™ã‚‹ï¼ˆSudachiï¼‰

```python
from sudachipy import dictionary
from libs import tokenize_df, tokenize_text_sudachi

tokenizer = dictionary.Dictionary(
    config_path="path/to/sudachi.json"
).create()

def my_tokenize_fn(text):
    return tokenize_text_sudachi(
        text,
        tokenizer=tokenizer,
        split_mode="B",
        word_form="normalized"
    )

df_tokens = tokenize_df(
    df,
    tokenize_text_fn=my_tokenize_fn
)
```

---

## 9.5 ã¾ã¨ã‚

- ãƒ¦ãƒ¼ã‚¶ãƒ¼å®šç¾©è¾æ›¸ã¯ tokenizer ä½œæˆæ™‚ã«æŒ‡å®šã™ã‚‹
- tokenize_df ã¯ tokenizer ã‚’å·®ã—æ›¿ãˆã‚‹ã ã‘ã§å¯¾å¿œã§ãã‚‹
- split_mode ã‚„ word_form ã‚’å¤‰ãˆãŸã„å ´åˆã¯ tokenize_text_fn ã‚’ä½¿ã†
- preprocess.py ã‚’å¤‰æ›´ã™ã‚‹å¿…è¦ã¯ãªã„

---

# 10. å…¨ä½“ã®ã¾ã¨ã‚

- `tokenize_df` ã¯å…¥å£ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ Janomeï¼‰
- ã¾ãšå…¨éƒ¨ tokenize â†’ çµæœã‚’è¦³å¯Ÿ â†’ `filter_tokens_df` ã§æ®µéšçš„ã«è½ã¨ã™
- Sudachi å›ºæœ‰ã®èª¿æ•´ï¼ˆ`split_mode`, `word_form`ï¼‰ã¯ `tokenize_text_fn` çµŒç”±ã§è¡Œã†
- åˆ¥ã‚¨ãƒ³ã‚¸ãƒ³ã‚’ä½¿ã†å ´åˆã¯ã€`tokenize_text_fn` ãŒè¿”ã™å½¢å¼ï¼ˆlist of 3-tuplesï¼‰ã‚’å®ˆã‚‹
